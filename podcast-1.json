{"podcast_title": "The TWIML AI Podcast (formerly This Week in Machine Learning & Artificial Intelligence)", "episode_title": "Why Deep Networks and Brains Learn Similar Features with Sophia Sanborn - #644", "episode_image": "https://megaphone.imgix.net/podcasts/35230150-ee98-11eb-ad1a-b38cbabcd053/image/TWIML_AI_Podcast_Official_Cover_Art_1400px.png?ixlib=rails-4.3.1&max-w=3000&max-h=3000&fit=crop&auto=format,compress", "episode_transcript": " All right, everyone, welcome to another episode of the TwiML AI Podcast. I am, of course, your host, Sam Sherrington. And today I'm joined by Sophia Sanborn. Sophia is a postdoctoral scholar in the Department of Electrical and Computer Engineering at the University of California Santa Barbara. Before we get going, be sure to take a moment to hit that subscribe button wherever you're listening to today's show. Sophia, welcome to the podcast. Thanks so much for having me on. I'm looking forward to digging into our conversation. This will be an interesting one, a bit interdisciplinary, starting from your background, which is interdisciplinary, but we'll be touching on some of the work you're doing in neuroscience and building neuroscience models using artificial neural networks. But why don't you get us started by sharing a little bit about your background and how you came to work in the field? Yeah. Well, so to make sense of my background, maybe let's start with my motivation, the big picture questions that really drive my research. So I've always been fascinated by the concept of a representation, the phenomenon of representation. Living systems can be differentiated from non-living systems in that living systems represent the external world. So information is coming in through sensors, it's being transformed, it's being encoded in chemical or electrical information and stored. Somehow all of the relevant world, information of the world out there is encoded in that information. And essentially our brain is transforming photons that hit the retina or oscillations of air pressure that hit the cochlea into this electrical activity, which somehow has this rich perceptual experience associated with it, this rich cognitive experience. So that's always really fascinated me. And that's been the big picture question that's driven my curiosity since the beginning of my undergraduate research. So yeah, since the beginning of my undergraduate studies, that's really the big picture question that has been driving me. As an undergraduate who is interested in big questions like this, I was naturally drawn to the philosophy department. Oftentimes people are a little bit surprised when they know of my research now in geometric deep learning, computational neuroscience, they're a little surprised to learn that my background is in philosophy. Sometimes people don't realize how deep the connection is between analytical philosophy and the origins of computer science. So a lot of what I learned in my undergraduate education was this long history of people who were thinking about this problem of representation and intelligence and trying to formulate that in terms of formal models, the formal mathematical models that could characterize that behavior. So as an undergrad, I was really studying this question more from a symbolic perspective. You know, there's the early 20th century thinkers like Paul Shannon, Alan Turing, Kurt Gertel, Virgin Russell, Gottlieb Frege, people who were trying to come up with these formal systems. And of course, this gave rise to the Turing machine, you know, Shannon's notion of information theory. In my undergrad studies, I was tackling it at this level of symbolic. And this naturally led me to start a PhD in computational cognitive science. But along the way, it started to kind of drive me crazy that I, you know, this symbolic level of abstraction for thinking about an intelligence doesn't really give you a handle on physically how this information is represented. So the way I posed the question at the beginning was really more of a biological question. And this sort of symbolic approach can't give you an answer that goes all the way down to the physics. I started my PhD at Berkeley thinking more from this background of symbolic reasoning. And then I found myself drawn to the Redwood Center for Theoretical Neuroscience, which is a group of engineers, physicists, biologists, and mathematicians who are studying principles of neural computation from the perspective of how information is really encoded in neurons. And that really drew me in. And I ended up switching my PhD to study computational neuroscience along the way. This was sort of the early days of neural networks and deep learning really catching on. So this was 2015. AlexNet came out in around 2012. And you know, what used to be kind of alternative perspective of connectionism started to be the main deal in machine learning and AI. So naturally, I was simultaneously studying deep learning and really trying to dig into the mathematics of it and understand how we can go beyond linear algebra, which gives us one set of tools for representing information to richer mathematical systems that can represent more structured information like the geometric structure that we have in the world. This is maybe a bit of a digression, but you mentioned analytical philosophy in there. What is that? So in Western philosophy, there's sort of two main traditions. There's analytical philosophy and continental philosophy. And the analytical philosophers were a group of philosophers who were trying to understand meaning from a mathematical and formal sense. So they were a bit obsessed with this question of if you could write formal statements that are completely true, like with logic. This was the development of a lot of the more advanced formal logic came out of analytical philosophy. And as I mentioned, those developments were happening sort of in parallel with the early computer science, the origins of computer science with Alan Turing and Claude Shannon. So analytical philosophy is really about trying to come up with a mathematical, rigorous way of communicating, of translating ideas, meaning into symbols. The continental philosophers were sort of the older tradition of philosophy that wasn't as concerned with precision in that sense. And so the way in which ideas in continental philosophy are communicated is a bit more poetic. The analytical philosophers were sort of railing against that and saying, well, I can't even really tell what you're saying. It's too vague. It's too much analogy here. We need to get very precise. So can you maybe give us some concrete examples of papers or research projects and how all these ideas that have formed your background kind of come to the fore in concrete research? Yeah, absolutely. So maybe I'll start with something that I found quite inspiring for the question that originally drove me back when I joined the Redwood Center. So when I joined the Redwood Center, I learned about the research of my PhD advisor, Bruno Olshausen. Some of the work that he did was on this problem, trying to understand principles of neural representation. There's this old idea in neuroscience, which is that a fundamental principle that organisms need to abide by is the notion of efficiency. So biological systems are fundamentally resource constrained. Biological systems can't just transmit information arbitrarily. There's a drive towards just sending only what is essential because every spike that you send in a neuron comes with a metabolic cost. So there was this idea that brains might be optimized not just to encode information in any arbitrary way. There's so many different ways that you can encode the same amount of information. Think about how we encode it in a computer in terms of 16 bit numbers or whatever versus the way that we're encoding them in physical, biological substrate. There's many different ways to do it. So maybe one way to constrain that is that biology is optimized to be as efficient as possible. If you look at the way that brains transmit information, the activity of brains is highly sparse, meaning that neurons are mostly off all of the time. They send a spike very infrequently. Of course, that would be what you would try to do if you were trying to be optimal in that sense. So what's really fascinating is that there's this very old finding from the 50s, the neuroscientists Hubel and Wiesel, who won a Nobel Prize for their discoveries back in these days. And they found that there are neurons in the primary visual cortex, which look like they're basically feature detectors. This finding is really like the foundation of all of neural networks now, this basic idea that individual neurons in the brain are representing semantically meaningful features. And those features in the primary visual cortex are looking for oriented edges. So they found it was actually an accident how they discovered this. They basically were doing these experiments with kittens, kind of brutal, cutting open the heads of kittens, stuffing electrodes in there, and showing these kittens different visual stimuli. And allegedly, they were looking for all kinds of things that would make the neurons fire. They were having a hard time doing it. But at some point, they accidentally moved the slide sheet, just the edge of the piece of paper, past the visual field. And then all of a sudden, they found this neuron that was just lighting up like crazy. And what they discovered is that there are neurons that were selective for edges that were oriented in a particular direction. And in fact, those neurons are selective not just for a particular orientation, but also for a particular width of edge. And so this was kind of a groundbreaking finding because up until that point, you know, we were really having a hard time getting a handle on how neurons were encoding information. And this was a kind of nice story is that individual neurons are looking for different features. So you can imagine building up now a hierarchy of visual representation. This is exactly what deep convolutional neural networks do, starting from edges and going to more complicated features like curves and then circles and all the way at the top of the hierarchy as something like a cat neuron or a face neuron. And as a very crude approximation, that's sort of what visual neuroscientists over the next 50 years discovered is happening in the brain, roughly, of course, with many caveats. But anyway, so those neurons, these oriented edge detectors, that's sort of a canonical finding from neuroscience. Now one explanation for what they're doing is that those neurons are looking for edges. And you know, you can kind of come up with a reason why that makes sense. Sure, the world is composed of things that have edges. It's not really a principled explanation. It doesn't really help us come up with predictions for what other kinds of features we might expect neurons to be looking for. Bruno Olshausen, my PhD advisor, he did this work in the 90s, early 2000s, showing that if you use that idea of efficiency, of efficient coding, this fundamental resource limitation as the guiding principle to constrain representation learning, then those features naturally emerge. These oriented edge detectors naturally emerge. So basically, what he did was he took a very simple neural network, artificial neural network, just a single one layer, you know, linear transformation followed by a non-linearity. And he trained the network to encode sort of natural images, things, you know, things you would see in nature, trees and whatever, just a pretty diverse set of stimuli. If you just try to encode that information into a compressed representation where your neurons are spiking infrequently, so it's sparse, and then you're just trying to reconstruct the input. So basically preserving the information, but encoding it in a sparse way. So doing a transformation that preserves only what is essential, then you learn exactly those features. We have ways of sort of visualizing what individual neurons in the brain are selective for. We call it the receptive field. So if you visualize that and then you look at what falls out of this algorithm, they're pretty much identical. It's really striking. So that is striking. It seems like there could be many different ways that it sounds like a dimensionality reduction and it seems like there could be many different representations and that the representation that earlier research has found is what's happening in the brain is what comes out of this algorithm is pretty interesting. Precisely. No, for me, I found that absolutely fascinating because why would it be exactly the same with the relatively loose constraint of essentially solving, just as you put it, this dimensionality reduction problem. We discover the same solution that the brain discovered through evolution. That means that there's something that's really fundamental about the constraint we've put on the system. Potentially the constraint we've put on that system is the same one that's guided the evolution of the brain. But just as you said, you would think that there's more than one solution to that problem. So that's quite mysterious. And that's a big theme that's in my work is this notion of universality because this has been happening over and over in both understanding neural representations and then also understanding how deep networks work is that consistently the same kinds of features show up in both of these systems. There's ways of doing mechanistic interpretability research on deep neural networks. You can take individual neurons in the deep network and we basically do neuroscience on it, but now we have the gradients so we can do back prop all the way to the input and say what's the input that's going to maximally stimulate this neuron? That's exactly like the artificial version of the neuroscience experiment with the kittens. We say what's the stimulus that I can create that's going to maximize the activity of this neuron? And when you do that for neurons in the deep network, these features show up that are strikingly consistent across different deep networks that are actually trained for different tasks. So if you do a lot of different convolutional neural networks, let's say that are trained, ones trained for classification, ones maybe a generative model, they're trained on different data sets, they have different sizes, the same features keep showing up. And then they keep, you know, there's this continued consistency with the kinds of things we see in biology. So that means that it's far from random and that there are these principles that are guiding this whole process. And what's more is that these features are highly mathematically structured. So in particular, the edge detectors that I told you about, these are actually the way that people started modeling them mathematically in the 80s was using Fourier analysis. What they realized is that they aren't just edges. So you think about how you sort of model an edge mathematically, it's sort of like black on one side and white on the other, right? You're looking for something that's positive, you know, high stimulation and then low stimulation. There's this change in contrast, but it's not as simple as that with these edge detectors. What researchers found out is that the behavior of these neurons can be very accurately modeled by a wavelet or a Gabor function. What that is, is basically you take a two-dimensional Fourier plane wave. So that kind of looks like a sine wave in 2D, kind of like the curtains that are behind you. So you take a 2D Fourier basis and then you localize it by putting a Gaussian window around it. And there's basically this oscillatory dynamics. So it kind of goes up and then it goes down. And so if you were to shift something left to right over a localized plane wave like that, the activity of the neuron, its response is sinusoidal. So it kind of goes up and then it goes down and goes back. That's exactly how these neurons behave. So mathematically, they seem to be using something like this Fourier analysis. Similarly, the features that are showing up in these deep networks, we get these beautiful Fourier-based features, Gabor features, and other things that have very striking mathematical structure and symmetries. And now things like Fourier were not developed at all in the context of biology, if memory serves. They were developed in the context of radar. Well, wavelets in particular, I think, was radar systems, maybe like World War II timeframe or something like that, if I'm remembering this correctly. I could be making this all up. I'm not sure about the history of wavelets in particular, but Fourier analysis was developed by Joseph Fourier, mostly in the context of thinking about physics and heat. So thermodynamics. And there's actually a very deep reason why all of this, the same math can be used to model physics and to model visual information. And we can get into that in a little bit. I know that wavelets have a long history, but they've been used in computer vision for quite some time. They were not developed in the context of neuroscience, but then neuroscientists realized that exactly that math described what neurons in the brain were doing. I could be hallucinating like a LLM. I remember something. I studied a bit of DSP and wavelets in grad school, and I thought I remembered something about radar systems as maybe it was one of the uses as opposed to the impetus for it. But in any case- Oh, I'm sure you're right about that. These were models that weren't created in the context of biology. They just happened to work really well once applied to biology. Yeah, exactly. And probably why that is, is because a lot of the problems, engineers were developing wavelets for doing signal processing. And a lot of the problems that they were trying to solve and that wavelets seemed to be a solution for, well, those are actually similar problems to the problem that biology is trying to solve. Biology is also doing signal processing. It's taking in information and trying to figure out what an optimal transform is to put it into a good representational space. So you can think of it sort of as convergent evolution. Engineers designed wavelets in biology, also designed wavelets in a parallel way. Yeah. That was kind of some of your advisor's work or colleagues' work in context to some of your research as well. You were getting to your research in the field. Yeah, exactly. So that was, for me, a very inspiring example of an approach towards deriving what we observe as emergent properties in these systems, biology, artificial networks, from greater principles, which actually give us a handle on what the system is trying to do. Now, when I sort of learned about these results, the thing that was striking to me is that Bruno and a lot of the other people who were doing this work were thinking about it in terms of compression, efficient coding, dimensionality reduction. But actually, there's this deeper mathematics that's underlying those same properties. So the Fourier transform, a lot of people learn about the Fourier transform in the context of engineering as a tool for signal processing. We learn it's a way to transform something like audio into its component frequencies and we can find what the coefficient is on each of the sine waves. So we think about it more from an engineering perspective as a tool. Fourier analysis actually has its really deep roots in the mathematics of group theory. Group theory is an area of abstract algebra, which is all about modeling transformation structure. So a lot of the kinds of transformations that we observe in the world, such as two dimensional rotation, three dimensional rotation, translation, scaling, stretching, squishing, all of these things, mathematically, these are describable as what are called groups. Groups are essentially just sets of objects and some composition. And then there's these constraints on the way that those objects can be composed. That's a very abstract definition. But yeah, so a group is essentially a set of transformations. So a good example is like the set, the group which is called SO3, the special orthogonal group of dimension three, which can be represented with 3D rotation matrices. So a group is just the set of all possible rotation matrices. And those matrices can be composed with matrix multiplication. And then for something to be a group, it has to adhere to certain axioms. So the operation has to be associative. There has to be an identity element, which is of course the identity matrix, which basically is the do nothing operation. If I apply that, nothing changes. There needs to be an inverse for every element of the group. So every rotation matrix has to have one that undoes it, and the group needs to be closed. So that means that if I were to compose any two elements, I would get another element from the set. So with rotations, you can combine one rotation and another, and you're always going to get a valid rotation. So groups are essentially the mathematics to describe much of the kinds of transformations that we observe in nature. Groups were used heavily in early 20th century physics. They provided a way of unifying the models of the fundamental forces and particles, and especially the mathematics of Lie groups, which are infinite groups, continuous groups, such as rotation. So interestingly, the Fourier transform is related to group theory, because essentially what we're doing when we take a Fourier transform is we are, I'm going to use some technical language here, but I can unpack it, we're projecting a signal onto what are called the irreducible representations of the group of translation. So now I use this term irreducible representations. A group is inherently naturally an abstract thing, but it can be made concrete by mapping it into linear algebra, and that's what a representation is. So a representation is essentially a mapping from an abstract group to matrices. Like I just said, we can make the notion of 3D rotation concrete by representing it with sets of 3D matrices, 3D rotation matrices. So the irreducible representations are just sort of a special irreducible set of those representations, there's many different ways to represent a group. They're sort of a very fundamental signature of an object, and the Fourier transform is using the representations of translation. So there's a well-known property of the Fourier transform, which is that it is what's called equivariant to shift, it's the Fourier shift theorem is sort of a very fundamental idea. If you were to take your signal and then take its Fourier transform, sorry, if you were to shift your signal and then take its Fourier transform, that's equivalent to taking the Fourier transform and then shifting it in a certain sense. This is this notion of equivariance. Have you come across this idea of equivariance? It's been coming up recently, especially in geometric deep learning and extensions of convolutional neural networks. Yeah. So it's based on exactly that mathematics. So what was interesting to me is this suggests potentially an alternative explanation for what these neurons in the visual cortex are trying to do. You could say, okay, they're compressing it, and that's why we end up seeing these wavelets, the wavelets provide a good basis for compression. Or alternatively, you could say, actually, maybe these neurons are trying to model the transformation structure. So because this particular feature is equivariant to translation, it's basically converting transformation out in the world, which is sort of, if you just get it in terms of raw pixel intensity, it's in a form that's difficult to model because it's kind of dispersed all over the retina. Like your whole image is getting translated left to right. But once you take like essentially a Fourier transform or a localized version of that, now you've converted the translation into this continuous change on the activity of a single neuron in a way that makes it really easy to model and generalize that information. So I got really interested in understanding basically an alternative explanation for those features in terms of groups. And that led me to this recent paper of mine, which was published at ICLR this year called Bispectral Neural Networks, in which we basically, you can think of it as doing what Bruno was doing in the sparse coding work, but for the idea of trying to capture transformation structure. So there he was saying, OK, if we use efficiency as the fundamental principle and we try to learn features from the data, then these things emerge. Well, we said, let's use invariance as the fundamental principle, because this is a very core problem that the brain needs to solve, is to form invariant representations. So we need to be able to model that transformation structure that's out there in the world and then also remove it so that we can recognize objects when they're on all of these different poses and things like that. So in this paper, we construct an artificial neural network with a particular structure. And there's some sort of, we'll put a tag on that in terms of what that is precisely. But we use this downstream objective of invariance to try to learn interesting representations. And what's really fascinating is that what falls out of that problem. So we're basically, by the way, the invariance objective is, you can really think of a contrast of learning, which is very popular as sort of learning scheme for deep networks. Contrast of learning is essentially doing an invariance objective. So it's saying, take all of these things, which we're going to say are the same. We don't really know how they're the same, but we're going to consider them to be the same. We want to map those to the same point in output space, and we want that point to be different from this other class of things, which are the same. So that's the contrast of framing. So we're basically using that kind of setup, but we're doing it in this really simple, very controlled network. And we want to see under these constraints, do we end up learning these fundamental objects from group theory that we predict that you should learn because they essentially allow the system to make explicit the transformation structure and then remove it in a way that's precise so that you can be invariant. And what we find is that indeed you do. What exactly are you applying that invariance constraint to? The output of some network? Exactly, the output of the network. So just as you would do for contrast of learning. So let's say I have some big data set of things. I pass it through the network. I get some transformation. I just want the output of my network to be invariant to the... When you do it in contrast of learning, it's basically to the class label. So you want everything that's within the same class to be equivalent. Here what we're doing is we're taking images that have been transformed according to some kind of group transformation, like rotation or translation. And we say, we want those to be going to the same point. For translation, 2D translation of images, which are just sort of shifting like this, we end up learning exactly the 2D Fourier basis, precisely, perfectly, like canonically, every single 2D Fourier basis function. It's perfect. It's so perfect that we can actually recover the structure of the group from it. Because there's this correspondence between representations, irreducible representations of a group and group elements themselves, we can use the learned weights to actually go back and read out what's called the group's Cayley table, which is the fundamental structure of the group. The Cayley table is essentially like a grownup multiplication table for abstract algebra, where it's saying if I take two group elements and I take their composition, which other group elements am I going to get? So for multiplication, it's like two times two equals four. For groups, it's group element one times g2 gives me g3, let's say. If you can get a model of the... What does it mean to learn the Fourier representation? Is that a three-dimensional transformations or a three-dimensional matrix? Is this a 2D matrix and it's a 2D matrix that's learned in the weights of the network or something? Yeah, precisely. So the Fourier transform is... Let's talk about the 1D Fourier transform first. It's just a linear transforms where basically you could write it abstractly as y equals wx, where w is some matrix and x is some incoming vector. The way that people implement the discrete Fourier transform in computers, w is this matrix of sine waves of different frequencies. So the way that we do it in this project is we say we don't know what w is because that's what we want to learn. We want to learn what the group is that structures the data. So we're going to start that out as random parameters. And as I said, we're going to use that invariance constraint as the loss, the output of the network. And now we're going to optimize w and see what happens. Now in this particular case, we... So y equals wx, that's sort of one linear transformation. As I said, the Fourier transform is equivariant, it's not invariant. So the Fourier transform actually doesn't eliminate the transformation structure. It preserves it and it makes it explicit because as something transforms, then it corresponds to a rotation in the output of the linear transformation in terms of the complex valued coefficients. So in order to construct an invariant from that, you need to somehow cancel out those phase shifts. And that's what this object called the bispectrum does. And that's where the name of the paper comes from, Bispectral Neural Networks, is the bispectrum precisely cancels out those transformations we get on the different coefficients to be perfectly invariant. So no matter what translation you apply, the output of the bispectrum is going to be exactly the same. What's interesting about the bispectrum is that there's many ways of constructing an invariant map. So if you think about common ways that people achieve invariance in deep neural networks, pooling is taking a max or an average. Pooling is usually an attempt to make things a little bit more invariant to small perturbations. And if you take a max or an average, that is indeed invariant, but it also destroys the signal structure. So if you think about two images, you could have a six in one direction and a six in a different direction. You take the average, it's going to be the same. But you can also just scramble the pixels of one of them and you take the average and that's also going to be the same. So in that sense, you could come up with adversarial examples for this max operator that it would think that were the same and that really aren't. So the bispectrum is special in that it is invariant, but it is also complete. It preserves all the signal structure. If you were to take the bispectrum of that scrambled image of the six, it would be completely different. So we use that basically the network has... Is the bispectrum network, is that a second result of this work in addition to this idea that if you train this model, then it will identify the Fourier coefficients or is the bispectrum network the thing that does that? The bispectrum network is the thing that does it. So the problem that we wanted to solve is to see, can we learn the group? Can we learn an unknown group from data for just observing, transforming things? Can I tell you, what is the transformation, its rotation, its translation? We're transforming things according to the rules of this group. Can we learn the group math essentially in the network? Exactly. Can we recover what the group is? It's a really hard problem and there are very few papers that have successfully done that. And so we're offering one way of doing that, which is to construct a network that basically does something very simple, linear transform, followed by this bispectral computation, which is basically assuming if I did have a Fourier transform in the first layer, we don't when we start out because it's random. But if I did, then I would want to combine the coefficients, the output of that layer in a particular way to cancel out the phase shifts due to the transformation. And then I would be perfectly invariant in a robust way. That's the structure of the bispectral network. Meaning you set up your objective function in such a way that it assumes this property and you train the model accordingly, and that's what enforces the property and gives you these coefficients in the model. Yes, exactly. Yeah, interesting. And so how do you see this research being applied? There is a sort of emerging line of work coming from maybe the last five or so years, five to seven years in geometric deep learning. So this line of work kind of was motivated by the idea of generalizing convolution to other groups. And it's using that same correspondence that I mentioned, this deep, you know, the roots of Fourier analysis in group theory. Sort of there's this duality between convolution and element-wise multiplication in the Fourier domain, they're equivalent. So anything we talk about in the Fourier domain, you can equivalently formalize in terms of convolutions. The reason why convolutional neural networks work so well on images is because baked into them is an assumption of translation, equivariance. A convolution is basically doing an operation which is equivariant to shifts. And as I said, we could formulate that equivalently as being a 2D Fourier transform for the group of 2D translations. Those are equivalent mathematically. And so geometric deep learning sort of arose from the desire to generalize that notion of a convolution to other groups of transformations so that we could potentially apply convolutions to domains besides images. So for images, translation is very relevant. But what if we want to do a convolution on something like a graph or on a curved surface? Let's say we want to do a convolution on a sphere. Then the equivalent of doing a convolution is going to be actually doing rotation, 3D rotation. Or if we wanted to be able to learn features in our networks, which are also robust to rotations, then maybe we want to do a rotational convolution on images as well. So this line of work has seen a good amount of success in that people have been able to generalize the concept of a CNN to all these different groups and thus different kinds of geometric domains. But in this line of work, all of those approaches assume that you know what the group is that structures your data. And oftentimes we don't actually know what that is. Maybe you have some sort of exotic group. So if you think about the kinds of transformations that structure differences, even if we just think about the domain of images, there's all these different transformations there that are relevant. And a lot of them are difficult to describe a priori. This is part of the reason why data augmentation can only get us so far. Data augmentation is pretty much necessary to get CNNs to generalize robustly to out of distribution data. But it requires the researcher to say exhaustively, okay, we're going to do rotations and translations and shifts and scaling and all of it. And you kind of apply that to your whole data set. And inevitably they're imperfect and you can't exhaustively list every possible transformation. And so this idea of learning the transformations is a way to augment that perspective. So geometric deep learning has shown it's really important to impose this structure in our networks. But I think it's also important to be able to parameterize and learn that structure. And so there's been a recent sort of interest in this problem of learning the group. And what we present in this paper in ICLR is one approach to doing that that's rooted in this Fourier analysis. And it can actually give us a complete description of the group in terms of the Cayley table of fundamental signature of the group structure. Is a research that demonstrates that if you enforce this kind of group structure in, for example, some kind of CNN, enhanced CNN, that you don't have to do the typical data augmentation kinds of things because it already knows about them? Yes. I'm pretty positive that that's been done. It's sort of theoretically sort of clear already that that's the case, because basically what we've done is we've exchanged, you know, your network is now basically doing the transformation instead of you doing it on the data. So like your network is now taking your filters and rotating them. So now you don't need to do that. I'm not sure of a paper. There probably is one that shows that explicitly. I don't have one off the top of my head. But that's the idea is that I guess the more interesting thing than the can it be done is whatever additional complexity that's happening in the network based on the constraint is that less expensive than multiplying all of your training data by N types of transformations that you use and like what that comparison is. Yeah, absolutely. It's a great question. So essentially what we have here is this trade off in terms of the complexity in the internal network structure, and then the complexity in what we've applied to the data. And then what that comes down to also is time complexity and how long it takes to train the system. I think it really depends on what your constraints are. So if you're trying to reduce training time, if that's the most important thing, then it can be worth it to put that additional structure into your network and to now sort of offload it to the complexity of the structure of the network. If training is cheap, which generally it's not, that's the really big expense, then you can just have a huge data set and do augmentation. But I think for most realistic cases, we want to really get that training time down and therefore it's worth it. Awesome. Wow, this is very heady stuff. I'm remembering a conversation with your colleague, Nina Mielen. We spoke, this is probably five years ago. And the example that she used that I thought was really interesting was like convolutions are designed to work on these 2D images, but what if in a medical context you have like a heart and it has this kind of curved three-dimensional shape. If you're doing a lot of processing on heart imagery, can you take advantage of the geometrical properties of that and either do better analysis or more efficient analysis in some way, enhancing or enhance what you're doing by taking advantage of that geometrical property? And it sounds like fundamentally you're trying to do similar things, but as opposed to enhancing an ML algorithm necessarily, you're trying to then use that to prove relationships between kind of the ML algorithms and the biological systems that they're modeling. Is that fair? Yeah, that's a great way of putting it. So essentially the idea is like the work that Nina Mielen does, which is incorporating geometric structure and geometric priors into machine learning algorithms. It's essentially saying, if we know that the structure is there, then there's going to be a more efficient way to compute it by exploiting that structure. Not just more efficient, but also more accurate. There's certain kinds of computations that you couldn't do correctly if you didn't know that your data were, for instance, lying on the surface of the sphere. If you tried to take the mean of those points, potentially the actually meaningful thing would be to give you a point that's on the sphere, but otherwise you would go to the center of the sphere. So having knowledge of that geometric structure is really essential for doing accurate and efficient machine learning for natural data that has this kind of rich geometric structure. But I guess for me, I've sort of come to a similar point. A lot of my research really is pure deep learning, machine learning, imbuing these algorithms with geometric or group structure to perform better on tasks. But for me, the connection, the motivation and background comes from really thinking from this more scientific perspective about biology, because that's not just an arbitrary strategy. It's not just something we can do because it sounds like a good idea. Actually the same constraints of needing to be efficient and accurate and because the world has this geometric structure, well, it means that biology actually evolved to do the same thing. So all of that rich mathematics, all of these differentiable Riemannian manifolds that Nina has programmed into geom stats so we can do machine learning on it, our brain in some sense also has representations of that structure. And so I've been fascinated by trying to understand how it is representing that mathematics is encoded in neural signals because that's the efficient, elegant solution to it. Yeah, it's interesting. Over the years, I've explored what I come to think of as the two-way street between neuroscience and machine learning where both fields are learning from the other, as well as exploring the idea of just how biologically inspired are neural nets. And I think this argument of essentially parallel evolution based on constraints is maybe the strongest case I've heard made for a biological grounding in neural networks. And I'm always very careful that you can go too far with the analogy and start to make assumptions about the one set of systems based on what we know about the other and everything falls apart. But the constraints driving biology to kind of use, what's the best way to put it? The constraints that are shared by both the biology and the mathematical models kind of drive some coalescence between the two is kind of a really interesting idea. Yeah, no, I think so as well. I think just like you, I'm absolutely wary of, you know, there's the classic analogy that everyone gives that, you know, if aerospace engineers were to really overfit to biology and trying to figure out this dynamics of flight, then we would be building planes with feathers on them, right? That's really not essential to the problem of flight. We can abstract away some details. And of course, there's so much that we abstract away in terms of details when we construct artificial neural networks, they're so different from biological neural networks. But they share some fundamental structure in that they're performing, you know, mathematical transformations on input, on inputs. And then if they have similar constraints, then yeah, we can expect that we might see a convergence in terms of solutions that they arrive at. Awesome. Well, Sophia, thanks so much for taking the time to share a bit about your research with us. Super fascinating stuff are there. You know, we'll of course include a link to the bi-spectral neural networks paper in the show notes. But are there other places that folks can go to learn more about this topic or your research or anything that you'd like to plug? Sure. I guess a couple things. One would be you can check out my website. It's just my first name, lastname.com, sophiasandborn.com. Has links to a lot of different papers and things I've been working on recently. But also, I'd like to plug two workshops that have been co-organizing. So one is the Topology, Algebra, and Geometry in Machine Learning workshop, which is happening at ICML next week. So if you're interested in the application of, you know, incorporating these geometric topological algebraic priors into machine learning algorithms, that's a whole workshop dedicated to that idea. We have some really awesome keynote speakers, including Tess Schmidt talking about Equivariant networks, Michael Bronstein. And then I'm also co-organizing a workshop at NeurIPS, which is Symmetry and Geometry in Neural Representations on, again, exactly this kind of line of ideas, but more specifically focused on that connection to neuroscience and on neural networks in particular, not just machine learning, generally speaking. So that'll be at NeurIPS in this winter. This is the second edition of it, and that'll be in December. And that's called NeurReps. So you can find that NeurReps.org. I'll give you some links to all of these different workshops and websites so that people can access them. Awesome. Thank you so much. All right, everyone, that's our show for today. To learn more about today's guest or the topics mentioned in this interview, visit TwiMLAI.com. Of course, if you like what you hear on the podcast, please subscribe, rate, and review the show on your favorite podcatcher. Thanks so much for listening and catch you next time."}